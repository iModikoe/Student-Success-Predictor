{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 04 – Insights & Explainability"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import json, joblib, warnings\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\nART = Path('../artifacts')\nmodel_path = ART / 'best_model.joblib'\nif not model_path.exists():\n    raise FileNotFoundError(\"Train a model first by running 03_Modeling.ipynb\")\n\npipe = joblib.load(model_path)\n\n# Try to extract feature importances\nclf = pipe.named_steps['clf']\npre = pipe.named_steps['pre']\n\n# Get feature names from preprocessor\ntry:\n    cat_features = pre.transformers_[1][2]  # names list\n    num_features = pre.transformers_[0][2]\n    # After fitting, OneHotEncoder has categories_\n    import itertools\n    ohe = pre.named_transformers_['cat']\n    cat_names = []\n    if hasattr(ohe, 'get_feature_names_out'):\n        cat_names = ohe.get_feature_names_out(cat_features).tolist()\n    feature_names = list(num_features) + cat_names\nexcept Exception as e:\n    feature_names = [f\"f{i}\" for i in range(0, 100)]  # fallback\n\n# Importance for tree-based models\ndef plot_importances(names, importances, title):\n    idx = np.argsort(importances)[::-1][:20]\n    plt.figure(figsize=(8,6))\n    plt.bar(range(len(idx)), np.array(importances)[idx])\n    plt.xticks(range(len(idx)), np.array(names)[idx], rotation=90)\n    plt.title(title)\n    plt.tight_layout()\n    plt.show()\n\nif hasattr(clf, 'feature_importances_'):\n    plot_importances(feature_names, clf.feature_importances_, \"Feature Importances (Tree-based)\")\nelse:\n    # Logistic regression: use absolute coefficients as a proxy\n    try:\n        coefs = np.abs(clf.coef_).ravel()\n        plot_importances(feature_names, coefs, \"Absolute Coefficients (LogReg)\")\n    except Exception as e:\n        warnings.warn(\"Could not compute feature importances/coefs. Consider SHAP below.\")\n\n# Optional: SHAP (works best for tree models or small samples)\ntry:\n    import shap\n    explainer = None\n    if clf.__class__.__name__.lower().startswith('xgb') or hasattr(clf, 'feature_importances_'):\n        explainer = shap.Explainer(clf, check_additivity=False)\n    else:\n        # KernelExplainer (slow) — sample few points\n        X_sample = np.random.randn(100, len(feature_names))\n        explainer = shap.KernelExplainer(clf.predict_proba, X_sample)\n    # Plot summary with random sample\n    shap.summary_plot = shap.summary_plot  # keep linter happy\n    print(\"SHAP is available. Use explainer to compute shap values on your transformed X.\")\nexcept Exception as e:\n    print(\"SHAP not available or failed. Skipping.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Summary**: Study time and absences typically correlate strongly with outcomes; prior grades (G1/G2) are also strong predictors. Use these plots to support your narrative."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}